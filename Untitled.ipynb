{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The rise of the internet has made it possible to collect massive amounts of data at a faster rate than ever, sometimes even in are a more controlled environment. For instance, it is quite easy nowadays to show two different versions of a product ot different customers simultaneously. This allow us to directly measure the impact of what these changes are, and has changed way products are designed. _This idea of showing two versions of a product at the same time is known as **A/B testing**_.\n",
    "\n",
    "The faster you can measure any impact of a change, the more useful these techniques are. In traditional A/B testing, this is usually achieved by increasing the number of people in the experiment, however there are a few variance reduction techniques that offer the ability to use additional information to increase the power of these experiments.\n",
    "\n",
    "In this post I'm going to look at one of these techniques: __control variates__, and suggest how we can improve it using machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic A/B Testing\n",
    "\n",
    "The goal of controlled experiments is often to estimate the Average Treatment Effect (ATE) of an intervention on some property, $y$, of a population. This is defined as\n",
    "\n",
    "$\\Delta = \\hbox{E}[y|I] - \\hbox{E}[y|\\bar{I}]$\n",
    "\n",
    "Where $I$ is an indicator for whether or not an individual received the intervention, and the expectation is averaged over the population.\n",
    "\n",
    "$\\Delta$ is the thing we fundamentally what to know - the effect of our treatment. However, because a single individual cannot both receive and not receive the intervention a the same time, we estimate this value by splitting a population into two groups at random, labeled $A$ and $B$, and applying our intervention only to group $B$.\n",
    "\n",
    "Our estimate of the ATE is then:\n",
    "\n",
    "$\\hat{\\Delta} = \\bar{y}_{B} - \\bar{y}_{A}$\n",
    "\n",
    "Where $\\bar{y}_{i}$ is the sample mean for group $i$. Splitting the samples into groups randomly ensures that no confounding variables influence our measurement.\n",
    "\n",
    "Because we only have a finite number of samples, there will be some error in this estimate. One way to quantify this error is the confidence intervals around $\\hat{\\Delta}$. The interpretation of confidence intervals is\n",
    "\n",
    "$P(\\hbox{True value} \\in \\hbox{confidence interval}) = 1 - \\alpha$.\n",
    "\n",
    "Where $1 - \\alpha$ is the coverage of the confidence interval, a value we choose to be between 0 and 1. Common values are 90% or 95%. The exact value you want will depend on how you are using your estimate. Note that here the true value is fixed, and it is the confidence interval which is a function of our data, and therefore a random variable.\n",
    "\n",
    "To estimate the confidence intervals for our estimator, we use the central limits theorem: $\\bar{y}_{i}$ is distributed normally with variance $\\hbox{Var}(y_{i})/N_{i}$ in the limit of large $N_{i}$.\n",
    "\n",
    "Under this approximation the standard deviation of $\\hat{\\Delta}$ is\n",
    "\n",
    "$\\sqrt{\\hbox{Var}(\\bar{y}_{B}) + \\hbox{Var}(\\bar{y}_{A})}$\n",
    "\n",
    "Which we can estimate from our data using\n",
    "\n",
    "$s_{\\Delta} = z \\sqrt{s_{B}^{2}/N_{B} + s_{A}^{2}/N_{A}}$\n",
    "\n",
    "Where $s_{i}$ is the sample standard deviation of group $i$. And the confidence intervals are just\n",
    "\n",
    "$se_{\\Delta} = z(\\alpha) \\sqrt{\\hbox{Var}(\\bar{y}_{B}) + \\hbox{Var}(\\bar{y}_{A})}$\n",
    "\n",
    "Where $z(\\alpha) = \\Phi^{-1}(1 - \\frac{\\alpha}{2})$ with $\\Phi^{-1}$ as the inverse normal CDF.\n",
    "\n",
    "We now have an estimate of the effect size, and our uncertainty of it. Often however, we need to make a decision with this information. How exactly we make this choice should depend on the cost/benefits of the decision, but it is sometimes enough just to ask whether or not our estimated value of $\\Delta$ is \"significantly\" different from zero. This is usually done by using the language of hypothesis testing.\n",
    "\n",
    "I don't want to go too much into the details of hypothesis testing, because I feel that confidence intervals are often a better way to present information, but since the language of A/B tests are often phrased as hypothesis tests, I should mention a few points:\n",
    "\n",
    "Hypothesis testing for a difference in means between two populations answers the specific question \"If we assume that the means of the two distributions are the same, is the probability I obtained my data smaller than $1 - \\alpha$?\". If the answer is yes, we say the results are \"statistically significant\".\n",
    "Here $\\alpha$ is the same $\\alpha$ we used for confidence intervals above. It is a parameter we as experimenters choose (and we choose it before the experiment starts). The larger $1 - \\alpha$ is, the more confident we are.\n",
    "If we ran a lot of A/A tests (tests where there is no intervention), we would expect $\\alpha$ of them to be \"significant\" ($\\alpha$ is sometimes called the false positive rate, or type one error).\n",
    "Once we have decided on a significance level, another question we can ask is: \"if there was a real difference between the populations of $\\Delta$, how often would we measure an effect?\". This value is called the statistical power, and is often denoted as $1 - \\beta$.\n",
    "If we are to run tests, it is in our best interest to make them as powerful as possible. The power of a test is a function of the sample size, the population variance, our significance level and the effect size. Because of this, it is intimately related to the confidence intervals. It turns out that for an effect size equal to the size of the confidence intervals, the power of a z-test is 50%.\n",
    "\n",
    "For the rest of these notes, I'm going to call an effect size \"detectable\" for a given experiment setup if it has power of at least 50%:\n",
    "\n",
    "$\\Delta_{detectable} \\ge se_{\\Delta}$\n",
    "\n",
    "I have made this term up to simplify these notes. It is not common statistical jargon, and in reality you should always aim for tests more powerful then this. \"Detectable effect size\" is a function of only the variance of the underlying population we want to measure, and the sample sizes of each group.\n",
    "\n",
    "Before continuing, I should note:\n",
    "\n",
    "I've made a lot of approximations here. There will be circumstances where they are all violated.\n",
    "One specific assumption is the normality of the estimator $\\hat{\\Delta}$ - for large sample sizes this is a good approximation, but for small samples there are more powerful tests you can run.\n",
    "Hypothesis tests in one way to approach detecting differences between two groups. There are others.\n",
    "Statistical significance should not be confused with the actual significance of a result.\n",
    "When we make an intervention, we don't just change the expectation of a population, we may change the underlying distribution completely. Under our normal approximation, this means that the variance of our sample mean may change as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dataset\n",
    "Having waded through some maths, it is good practice to check our results by simulation. In doing so, we can confirm that even using our ideal assumptions we have an idea what's going on.\n",
    "\n",
    "To do this we need a mechanism we really generates data. The function below does this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    x = np.random.uniform(low=0, high=1, size=n_samples*2)\n",
    "    y = (\n",
    "        + 10 * np.abs(x - 0.25)  \n",
    "        + 2 * np.sin((x * 5) * 2 * np.pi)\n",
    "        + np.random.normal(size=n_samples*2)\n",
    "    )\n",
    "    \n",
    "    assignment = [1] * n_samples + [0] * n_samples\n",
    "    np.random.shuffle(assignment)\n",
    "    \n",
    "    samples = pd.DataFrame({\n",
    "        \"x\": x,\n",
    "        \"y\": y,\n",
    "        \"group\": assignment\n",
    "    })\n",
    "    \n",
    "    samples.loc[lambda df: df.group == 1, \"y\"] += uplift\n",
    "    \n",
    "    return samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
